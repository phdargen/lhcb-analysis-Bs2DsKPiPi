\section{Selection}

A twofold approach is used to isolate the $\Bs\to\Ds\kaon\pion\pion$ from data passing the stripping line. 
First, further one-dimensional cuts are applied to reduce the level of combinatorial background and to veto some specific physical background. 
After that, a multivariate analysis selection is performed, combining multiple variables to train a neural network and create a powerfull discriminator between signal and background. 

\subsection{Cut-based selection}

In order to minimize the contribution of combinatorial background to our samples, we apply the following cuts to the b-hadron:

\begin{enumerate}[(i)]

\item DIRA $>$ 0.99994

\item min IP $\chi^{2}$ $<$ 20 to any PV

\item FD $\chi^{2}$ $>$ 100 to any PV

\item Vertex $\chi^{2}$/nDoF $<$ 8 

\end{enumerate}    


Additionaly, we veto various physical beackgrounds, which have either the same final state as our signal decay, or can contribute via a single miss-identification of $\kaon\to\pion$ or $\kaon\to\proton$:

\begin{itemize}

\item $\Bs\to\Dsp\Dsm$ : $|M(\kaon\pion\pion) - m_{\Ds}| > 20 \mevcc$ 

\item $\Bs\to\Ds\kaon\kaon\pion$ : $\pim$ $\dllkpi$ $<$ 5 

\item $\Bz\to\Dp(\to\Kp\pim\pip)\kaon\pion\pion$ : possible with single miss-ID of $\Kp\rightarrow\pip$, vetoed by changing mass hypothesis and recompute $|M(\Kp\pim\pip) - m_{Dp}|$ $>$ 20 $\mevcc$, 
or the $\Kp$ has to fulfill $\dllkpi$ $>$ 10

\item $\Lb\to\Lc(\to\proton\Km\pip)\kaon\pion\pion$ : possible with single miss-ID of $\Kp\rightarrow\proton$, vetoed by changing mass hypothesis and recompute $M(\proton\Km\pip) - m_{\Lc}$ $>$ 15 $\mevcc$, 
or the $\Kp$ has to fulfill $\mathrm{DLL}_{\kaon\proton}$ $>$ 0  

\end{itemize} 


All signal candidates for the branching ratio measurement are reconstructed via the $\Ds\to\Kp\Km\pip$ channel. This decay can either proceed via the narrow $\phiz$ resonance, the broader $\Kstarz$ resonance, or non-resonant.
Depending on the process being resonant or not, we apply additional PID requirements:

\begin{enumerate}

\item resonant case, no additional PID requirements:
\begin{enumerate}
\item $\Dsp\to\phiz\pip$ ($|M(\Kp\Km) - m_{\phiz}|$ $<$ 20 $\mevcc$)  
\item $\Dsp\to\Kstarzb\Kp$ ($|M(\Km\pip) -m_{\Kstarz}$ $<$ 75 $\mevcc$)
\end{enumerate}

\item non-resonant case: $\dllkpi$ $>$ 5 for kaons

\end{enumerate}


%%%


\subsection{Multivariate stage}

We use TMVA \cite{Hocker:2007ht} to train a multivariate descriminator, which is used to further improve the signal to background ratio. The 17 variables used for the training are:

\begin{itemize} 

\item max(ghostProb) over all tracks

\item cone($\pt$) asymmetrie of every track

\item min(IP$\chi^{2}$) over the $X_{s}$ daughters

\item max(DOCA) over all pairs of $X_{s}$ daughters

\item min(IP$\chi^{2}$) over the $\Ds$ daughters

\item $\Ds$ DIRA

\item $\Ds$ FD significance

\item max($\cos(\Ds h_{i})$), where $\cos(\Ds h_{i})$ is the cosine of the angle between the $\Ds$ and another track i in the plane transverse to the beam

\item $\Bs$ IP$\chi^{2}$, FD$\chi^{2}$ and Vertex $\chi^{2}$

\end{itemize}

Various classifiers were investigated in order to select the most efficient discriminator. As the result a boosted decision tree with gradient boost (BDTG) is chosen as nominal classifier. 
We use truth-matched Monte Carlo (MC), taken from the mass region $\pm 60 \mevcc$ around the nominal $\Bs$ mass, as signal input. 
Those simulated signal candidates are required to pass the same trigger and stripping requirements, that were used to select the data samples. 
For the background we use events from the high mass sideband ($m_{\Bs candidate}$ $>$ 5600 $\mevcc$) of our data samples. \newline
The distributions of the input variables for signal and background are shown in Fig. XXX. 
The BDTG output distribution for test and training samples is shown in Fig YYY. No sign of overtraining is observed. The efficiency curves as a function of the cut value are shown in Fig. ZZZ. \newline
The relative importance of the input variables for the BDTG training is summarized in Table FFF. 
       
